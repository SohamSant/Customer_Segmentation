{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 1 - Our Goal : \n","\n","In this project, we will use K-means clustering and PCA to perform customer segmentation based on their purchasing behavior in an e-commerce dataset. The goal is to identify distinct groups of customers with similar preferences and behaviors, enabling personalized marketing strategies and recommendations. \n","\n","Extract relevant features from the dataset that capture customer behavior, such as purchase history, order frequency, total spending, etc. Calculate additional metrics if necessary, such as recency of purchase or average basket size. Dimensionality Reduction with PCA: Apply PCA to reduce the dimensionality of the feature space while retaining the most informative features\n","\n","Determining Optimal Number of Clusters: Use the elbow method or silhouette analysis to determine the optimal number of clusters for K-means. Experiment with different values of K and evaluate the clustering results. K-means Clustering: Perform K-means clustering on the reduced feature space. Assign each customer to a cluster based on their feature values. Analyze the resulting clusters and interpret the characteristics of each segment. Cluster Profiling: Profile each cluster by calculating cluster-specific metrics, such as average spending, purchase frequency, or popular product categories. Identify the key characteristics and behaviors that distinguish each cluster. Visualization: Visualize the clusters and their separation using scatter plots or other suitable techniques. Plot the clusters based on the reduced feature space to understand the distribution and overlap of customers\n","\n","Evaluate the quality of the clustering results using appropriate metrics such as silhouette score or within-cluster sum of squares (WCSS). Assess the cohesion and separation of the clusters to determine the effectiveness of the segmentation. Personalization and Recommendations: Based on the identified customer segments, develop personalized marketing strategies and recommendations. Tailor promotions, product suggestions, or communication channels for each cluster to enhance customer engagement and satisfaction.\n","\n","Interpret the results and provide insights about the different customer segments. Discuss the implications for the e-commerce business, such as targeted marketing, customer retention, or inventory management. Remember to adhere to ethical guidelines and data privacy regulations while working with customer data.\n"]},{"cell_type":"markdown","metadata":{},"source":["# 2 - import important Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:25.476283Z","iopub.status.busy":"2023-09-27T15:52:25.475866Z","iopub.status.idle":"2023-09-27T15:52:27.45959Z","shell.execute_reply":"2023-09-27T15:52:27.458535Z","shell.execute_reply.started":"2023-09-27T15:52:25.476252Z"},"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'plotly'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32md:\\COLLEGE\\TE\\customerSegmentation.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/COLLEGE/TE/customerSegmentation.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/COLLEGE/TE/customerSegmentation.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgridspec\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mgridspec\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/COLLEGE/TE/customerSegmentation.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgraph_objects\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mgo\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/COLLEGE/TE/customerSegmentation.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolors\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearSegmentedColormap\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/COLLEGE/TE/customerSegmentation.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m colors \u001b[39mas\u001b[39;00m mcolors\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"]}],"source":["# Ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","# !pip install yellowbrick\n","# !pip install numpy pandas seaborn matplotlib plotly scikit-learn yellowbrick tabulate\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","import plotly.graph_objects as go\n","from matplotlib.colors import LinearSegmentedColormap\n","from matplotlib import colors as mcolors\n","from scipy.stats import linregress\n","from sklearn.ensemble import IsolationForest\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n","from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n","from sklearn.cluster import KMeans\n","from tabulate import tabulate\n","from collections import Counter\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:27.461801Z","iopub.status.busy":"2023-09-27T15:52:27.461254Z","iopub.status.idle":"2023-09-27T15:52:27.661573Z","shell.execute_reply":"2023-09-27T15:52:27.660947Z","shell.execute_reply.started":"2023-09-27T15:52:27.461768Z"},"trusted":true},"outputs":[],"source":["%matplotlib inline\n","from plotly.offline import init_notebook_mode\n","init_notebook_mode(connected=True)"]},{"cell_type":"markdown","metadata":{},"source":["# 3 - Data collection - Display some infos"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:27.663155Z","iopub.status.busy":"2023-09-27T15:52:27.662544Z","iopub.status.idle":"2023-09-27T15:52:28.974414Z","shell.execute_reply":"2023-09-27T15:52:28.973307Z","shell.execute_reply.started":"2023-09-27T15:52:27.663086Z"},"trusted":true},"outputs":[],"source":["df = pd.redf = pd.read_csv('data.csv', encoding='unicode_escape')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:28.976628Z","iopub.status.busy":"2023-09-27T15:52:28.976374Z","iopub.status.idle":"2023-09-27T15:52:29.003702Z","shell.execute_reply":"2023-09-27T15:52:29.002582Z","shell.execute_reply.started":"2023-09-27T15:52:28.976605Z"},"trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:29.007443Z","iopub.status.busy":"2023-09-27T15:52:29.006624Z","iopub.status.idle":"2023-09-27T15:52:29.112027Z","shell.execute_reply":"2023-09-27T15:52:29.111146Z","shell.execute_reply.started":"2023-09-27T15:52:29.007392Z"},"trusted":true},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:29.11519Z","iopub.status.busy":"2023-09-27T15:52:29.114692Z","iopub.status.idle":"2023-09-27T15:52:29.181279Z","shell.execute_reply":"2023-09-27T15:52:29.180322Z","shell.execute_reply.started":"2023-09-27T15:52:29.115159Z"},"trusted":true},"outputs":[],"source":["df.describe().T"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:29.184563Z","iopub.status.busy":"2023-09-27T15:52:29.184263Z","iopub.status.idle":"2023-09-27T15:52:29.456345Z","shell.execute_reply":"2023-09-27T15:52:29.455493Z","shell.execute_reply.started":"2023-09-27T15:52:29.184535Z"},"trusted":true},"outputs":[],"source":["df.describe(include = \"object\").T"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:29.45941Z","iopub.status.busy":"2023-09-27T15:52:29.459126Z","iopub.status.idle":"2023-09-27T15:52:29.464852Z","shell.execute_reply":"2023-09-27T15:52:29.463979Z","shell.execute_reply.started":"2023-09-27T15:52:29.459386Z"},"trusted":true},"outputs":[],"source":["df.columns "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:29.465968Z","iopub.status.busy":"2023-09-27T15:52:29.465719Z","iopub.status.idle":"2023-09-27T15:52:29.481362Z","shell.execute_reply":"2023-09-27T15:52:29.48028Z","shell.execute_reply.started":"2023-09-27T15:52:29.465947Z"},"trusted":true},"outputs":[],"source":["df.shape"]},{"cell_type":"markdown","metadata":{},"source":["# 3 - Data Cleaning & Transformation"]},{"cell_type":"markdown","metadata":{},"source":["### 3-1 Handling missing values"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:29.48515Z","iopub.status.busy":"2023-09-27T15:52:29.484576Z","iopub.status.idle":"2023-09-27T15:52:29.577509Z","shell.execute_reply":"2023-09-27T15:52:29.576364Z","shell.execute_reply.started":"2023-09-27T15:52:29.485093Z"},"trusted":true},"outputs":[],"source":["missing_values = df.isnull().sum()\n","missing_values"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:29.579625Z","iopub.status.busy":"2023-09-27T15:52:29.578823Z","iopub.status.idle":"2023-09-27T15:52:29.58819Z","shell.execute_reply":"2023-09-27T15:52:29.586834Z","shell.execute_reply.started":"2023-09-27T15:52:29.579595Z"},"trusted":true},"outputs":[],"source":["missing_percentage = (missing_values[missing_values>0] / df.shape[0]) * 100\n","missing_percentage"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:29.590039Z","iopub.status.busy":"2023-09-27T15:52:29.589754Z","iopub.status.idle":"2023-09-27T15:52:29.910457Z","shell.execute_reply":"2023-09-27T15:52:29.909613Z","shell.execute_reply.started":"2023-09-27T15:52:29.590015Z"},"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(15, 4))\n","ax.barh(missing_percentage.index, missing_percentage, color='blue')\n","\n","\n","for i, (value, name) in enumerate(zip(missing_percentage, missing_percentage.index)):\n","    ax.text(value+0.5, i, f\"{value:.2f}%\", ha='left', va='center', fontweight='bold', fontsize=18, color='red')\n","\n","ax.set_xlim([0, 40])\n","\n","plt.title(\"Percentage of Missing Values\", fontweight='bold', fontsize=22)\n","plt.xlabel('Percentages (%)', fontsize=16)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:29.911888Z","iopub.status.busy":"2023-09-27T15:52:29.911599Z","iopub.status.idle":"2023-09-27T15:52:29.982059Z","shell.execute_reply":"2023-09-27T15:52:29.980905Z","shell.execute_reply.started":"2023-09-27T15:52:29.911864Z"},"trusted":true},"outputs":[],"source":["df = df.dropna(subset=['CustomerID', 'Description'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:29.983591Z","iopub.status.busy":"2023-09-27T15:52:29.983223Z","iopub.status.idle":"2023-09-27T15:52:30.052288Z","shell.execute_reply":"2023-09-27T15:52:30.050984Z","shell.execute_reply.started":"2023-09-27T15:52:29.98356Z"},"trusted":true},"outputs":[],"source":["# for Total sum\n","df.isnull().sum().sum()"]},{"cell_type":"markdown","metadata":{},"source":["### 3-2 Handling Duplicates"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:30.053921Z","iopub.status.busy":"2023-09-27T15:52:30.05359Z","iopub.status.idle":"2023-09-27T15:52:30.295221Z","shell.execute_reply":"2023-09-27T15:52:30.293801Z","shell.execute_reply.started":"2023-09-27T15:52:30.053891Z"},"trusted":true},"outputs":[],"source":["print(f\"The Dataset contains >>> ({df.duplicated().sum()}) duplicate rows that need to be removed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:30.297381Z","iopub.status.busy":"2023-09-27T15:52:30.296992Z","iopub.status.idle":"2023-09-27T15:52:30.561381Z","shell.execute_reply":"2023-09-27T15:52:30.56026Z","shell.execute_reply.started":"2023-09-27T15:52:30.297346Z"},"trusted":true},"outputs":[],"source":["df.drop_duplicates(inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:30.563233Z","iopub.status.busy":"2023-09-27T15:52:30.562344Z","iopub.status.idle":"2023-09-27T15:52:30.780936Z","shell.execute_reply":"2023-09-27T15:52:30.780234Z","shell.execute_reply.started":"2023-09-27T15:52:30.563202Z"},"trusted":true},"outputs":[],"source":["duplicate_rows = df[df.duplicated(keep=False)]\n","\n","if duplicate_rows.empty:\n","    print(\"No duplicate rows found.\")\n","else:\n","    print(\"Duplicate rows found.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:30.782668Z","iopub.status.busy":"2023-09-27T15:52:30.781893Z","iopub.status.idle":"2023-09-27T15:52:30.78856Z","shell.execute_reply":"2023-09-27T15:52:30.787252Z","shell.execute_reply.started":"2023-09-27T15:52:30.782641Z"},"trusted":true},"outputs":[],"source":["df.shape[0]"]},{"cell_type":"markdown","metadata":{},"source":["### 3-3 Check"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:30.791255Z","iopub.status.busy":"2023-09-27T15:52:30.790133Z","iopub.status.idle":"2023-09-27T15:52:30.806234Z","shell.execute_reply":"2023-09-27T15:52:30.804691Z","shell.execute_reply.started":"2023-09-27T15:52:30.791196Z"},"trusted":true},"outputs":[],"source":["print(df[\"Quantity\"].min())\n","print(df[\"UnitPrice\"].min())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:30.810007Z","iopub.status.busy":"2023-09-27T15:52:30.809406Z","iopub.status.idle":"2023-09-27T15:52:30.857479Z","shell.execute_reply":"2023-09-27T15:52:30.856012Z","shell.execute_reply.started":"2023-09-27T15:52:30.809964Z"},"trusted":true},"outputs":[],"source":["df = df.loc[df[\"Quantity\"] >0 ]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:30.859484Z","iopub.status.busy":"2023-09-27T15:52:30.859071Z","iopub.status.idle":"2023-09-27T15:52:30.866012Z","shell.execute_reply":"2023-09-27T15:52:30.864822Z","shell.execute_reply.started":"2023-09-27T15:52:30.859449Z"},"trusted":true},"outputs":[],"source":["print(df[\"Quantity\"].min())"]},{"cell_type":"markdown","metadata":{},"source":["### 3-4 Correcting StockCode Anomalies\n","\n","Stock Code Anomalies: We observe that while most stock codes are composed of 5 or 6 characters, there are some anomalies like the code 'POST'. These anomalies might represent services or non-product transactions (perhaps postage fees) rather than actual products. To maintain the focus of the project, which is clustering based on product purchases and creating a recommendation system, these anomalies should be further investigated and possibly treated appropriately to ensure data integrity."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:30.868536Z","iopub.status.busy":"2023-09-27T15:52:30.867564Z","iopub.status.idle":"2023-09-27T15:52:30.909256Z","shell.execute_reply":"2023-09-27T15:52:30.90799Z","shell.execute_reply.started":"2023-09-27T15:52:30.868499Z"},"trusted":true},"outputs":[],"source":["# Finding the number of unique stock codes\n","unique_stock_codes = df['StockCode'].nunique()\n","unique_stock_codes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:30.911016Z","iopub.status.busy":"2023-09-27T15:52:30.910595Z","iopub.status.idle":"2023-09-27T15:52:30.94913Z","shell.execute_reply":"2023-09-27T15:52:30.947958Z","shell.execute_reply.started":"2023-09-27T15:52:30.91099Z"},"trusted":true},"outputs":[],"source":["# Finding the top 10 most frequent stock codes\n","top_10_stock_codes = df['StockCode'].value_counts(normalize=True).head(10) * 100\n","top_10_stock_codes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:30.950447Z","iopub.status.busy":"2023-09-27T15:52:30.950108Z","iopub.status.idle":"2023-09-27T15:52:31.219752Z","shell.execute_reply":"2023-09-27T15:52:31.218789Z","shell.execute_reply.started":"2023-09-27T15:52:30.950416Z"},"trusted":true},"outputs":[],"source":["# Plotting the top 10 most frequent stock codes\n","plt.figure(figsize=(12, 5))\n","top_10_stock_codes.plot(kind='barh', color='blue')\n","\n","# Adding the percentage frequency on the bars\n","for index, value in enumerate(top_10_stock_codes):\n","    plt.text(value, index+.25, f'{value:.2f}%', fontsize=10)\n","\n","plt.title('Top 10 Most Frequent Stock Codes')\n","plt.xlabel('Percentage Frequency (%)')\n","plt.ylabel('Stock Codes')\n","plt.gca().invert_yaxis()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:31.22298Z","iopub.status.busy":"2023-09-27T15:52:31.22269Z","iopub.status.idle":"2023-09-27T15:52:31.256041Z","shell.execute_reply":"2023-09-27T15:52:31.254952Z","shell.execute_reply.started":"2023-09-27T15:52:31.222957Z"},"trusted":true},"outputs":[],"source":["unique_stock_codes = df['StockCode'].unique()\n","unique_stock_codes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:31.258605Z","iopub.status.busy":"2023-09-27T15:52:31.257573Z","iopub.status.idle":"2023-09-27T15:52:31.274968Z","shell.execute_reply":"2023-09-27T15:52:31.273646Z","shell.execute_reply.started":"2023-09-27T15:52:31.258561Z"},"trusted":true},"outputs":[],"source":["numeric_char_counts_in_unique_codes = pd.Series(unique_stock_codes).apply(lambda x: sum(c.isdigit() for c in str(x))).value_counts()\n","numeric_char_counts_in_unique_codes"]},{"cell_type":"markdown","metadata":{},"source":["Inference:\n","The output indicates the following:\n","\n","A majority of the unique stock code contain exactly 5 numeric characters, which seems to be the standard format for representing product codes in this dataset.\n","There are a few anomalies: 5 stock codes contain no numeric characters and 1 stock code contains only 1 numeric character. These are clearly deviating from the standard format and we will treat them as Outliers and we will remove them"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:31.276867Z","iopub.status.busy":"2023-09-27T15:52:31.276481Z","iopub.status.idle":"2023-09-27T15:52:31.293547Z","shell.execute_reply":"2023-09-27T15:52:31.292276Z","shell.execute_reply.started":"2023-09-27T15:52:31.276835Z"},"trusted":true},"outputs":[],"source":["anomalous_stock_codes = [code for code in unique_stock_codes if sum(c.isdigit() for c in str(code)) in (0, 1)]\n","\n","# Printing each stock code on a new line\n","print(\"Anomalous stock codes:\")\n","print(\"-\"*22)\n","for code in anomalous_stock_codes:\n","    print(code)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:31.300418Z","iopub.status.busy":"2023-09-27T15:52:31.300063Z","iopub.status.idle":"2023-09-27T15:52:31.331253Z","shell.execute_reply":"2023-09-27T15:52:31.330528Z","shell.execute_reply.started":"2023-09-27T15:52:31.30039Z"},"trusted":true},"outputs":[],"source":["# Calculating the percentage of records with these stock codes\n","percentage_anomalous = (df['StockCode'].isin(anomalous_stock_codes).sum() / len(df)) * 100\n","print(f\"{percentage_anomalous:.2f}%\")"]},{"cell_type":"markdown","metadata":{},"source":["Based on the analysis, we find that a very small proportion of the records, 0.39%, have anomalous stock codes, which deviate from the typical format observed in the majority of the data.So we will remove them\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:31.332506Z","iopub.status.busy":"2023-09-27T15:52:31.332165Z","iopub.status.idle":"2023-09-27T15:52:31.400399Z","shell.execute_reply":"2023-09-27T15:52:31.398537Z","shell.execute_reply.started":"2023-09-27T15:52:31.332472Z"},"trusted":true},"outputs":[],"source":["df = df[~df['StockCode'].isin(anomalous_stock_codes)]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:31.402332Z","iopub.status.busy":"2023-09-27T15:52:31.401975Z","iopub.status.idle":"2023-09-27T15:52:31.409278Z","shell.execute_reply":"2023-09-27T15:52:31.40831Z","shell.execute_reply.started":"2023-09-27T15:52:31.402302Z"},"trusted":true},"outputs":[],"source":["df.shape[0]"]},{"cell_type":"markdown","metadata":{},"source":["### 3-5 Treating Zero Unit Prices"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:31.410879Z","iopub.status.busy":"2023-09-27T15:52:31.410605Z","iopub.status.idle":"2023-09-27T15:52:31.440422Z","shell.execute_reply":"2023-09-27T15:52:31.4395Z","shell.execute_reply.started":"2023-09-27T15:52:31.410855Z"},"trusted":true},"outputs":[],"source":["df['UnitPrice'].describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:31.441793Z","iopub.status.busy":"2023-09-27T15:52:31.441491Z","iopub.status.idle":"2023-09-27T15:52:31.459589Z","shell.execute_reply":"2023-09-27T15:52:31.458286Z","shell.execute_reply.started":"2023-09-27T15:52:31.441764Z"},"trusted":true},"outputs":[],"source":["df[df['UnitPrice']==0].describe()[['Quantity']]"]},{"cell_type":"markdown","metadata":{},"source":["<b>Inferences on UnitPrice:<b>\n","\n","The transactions with a unit price of zero are relatively few in number (33 transactions).\n","These transactions have a large variability in the quantity of items involved, ranging from 1 to 12540, with a substantial standard deviation.\n","Including these transactions in the clustering analysis might introduce noise and could potentially distort the customer behavior patterns identified by the clustering algorithm.\n","\n","<b>Strategy:<b/>\n","    \n","Given the small number of these transactions and their potential to introduce noise in the data analysis, the strategy should be to remove these transactions from the dataset. This would help in maintaining a cleaner and more consistent dataset, which is essential for building an accurate and reliable clustering model and recommendation system."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:31.46114Z","iopub.status.busy":"2023-09-27T15:52:31.460809Z","iopub.status.idle":"2023-09-27T15:52:31.507091Z","shell.execute_reply":"2023-09-27T15:52:31.505787Z","shell.execute_reply.started":"2023-09-27T15:52:31.461109Z"},"trusted":true},"outputs":[],"source":["# Removing records with a unit price of zero to avoid potential data entry errors\n","df = df[df['UnitPrice'] > 0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:31.508701Z","iopub.status.busy":"2023-09-27T15:52:31.508426Z","iopub.status.idle":"2023-09-27T15:52:31.516065Z","shell.execute_reply":"2023-09-27T15:52:31.51468Z","shell.execute_reply.started":"2023-09-27T15:52:31.508679Z"},"trusted":true},"outputs":[],"source":["df.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:31.517928Z","iopub.status.busy":"2023-09-27T15:52:31.517592Z","iopub.status.idle":"2023-09-27T15:52:31.52799Z","shell.execute_reply":"2023-09-27T15:52:31.526036Z","shell.execute_reply.started":"2023-09-27T15:52:31.5179Z"},"trusted":true},"outputs":[],"source":["# Resetting the index of the cleaned dataset\n","df.reset_index(drop=True, inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["<b>In K-means clustering, the algorithm is sensitive to both the scale of data and the presence of outliers, as they can significantly influence the position of centroids, potentially leading to incorrect cluster assignments. However, considering the context of this project where the final goal is to understand customer behavior and preferences through K-means clustering, it would be more prudent to address the issue of outliers after the feature engineering phase where we create a customer-centric dataset. At this stage, the data is transactional, and removing outliers might eliminate valuable information that could play a crucial role in segmenting customers later on. Therefore, we will postpone the outlier treatment and proceed to the next stage for now<b>"]},{"cell_type":"markdown","metadata":{},"source":["# 4 - Feature Engineering"]},{"cell_type":"markdown","metadata":{},"source":["## 4-1 RFM Features\n","\n","RFM is a method used for analyzing customer value and segmenting the customer base. It is an acronym that stands for:\n","\n","<B>Recency (R):<B>\n","\n","This metric indicates how recently a customer has made a purchase. A lower recency value means the customer has purchased more recently, indicating higher engagement with the brand.\n","\n","<b>Frequency (F):<B>\n","\n","This metric signifies how often a customer makes a purchase within a certain period. A higher frequency value indicates a customer who interacts with the business more often, suggesting higher loyalty or satisfaction.\n","\n","<b>Monetary (M):<B>\n","\n","This metric represents the total amount of money a customer has spent over a certain period. Customers who have a higher monetary value have contributed more to the business, indicating their potential high lifetime value.\n","\n","<b>Together, these metrics help in understanding a customer's buying behavior and preferences, which is pivotal in personalizing marketing strategies and creating a recommendation system.<B>"]},{"cell_type":"markdown","metadata":{},"source":["### 4-1-1 Recency (R)\n","\n","In this step, we focus on understanding how recently a customer has made a purchase. This is a crucial aspect of customer segmentation as it helps in identifying the engagement level of customers. Here, I am going to define the following feature:\n","\n","Days Since Last Purchas: This feature represents the number of days that have passed since the customer's last purchase. A lower value indicates that the customer has purchased recently, implying a higher engagement level with the business, whereas a higher value may indicate a lapse or decreased engagement. By understanding the recency of purchases, businesses can tailor their marketing strategies to re-engage customers who have not made purchases in a while, potentially increasing customer retention and fostering loyalty.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:31.529422Z","iopub.status.busy":"2023-09-27T15:52:31.529114Z","iopub.status.idle":"2023-09-27T15:52:31.715181Z","shell.execute_reply":"2023-09-27T15:52:31.713542Z","shell.execute_reply.started":"2023-09-27T15:52:31.529397Z"},"trusted":true},"outputs":[],"source":["# Convert InvoiceDate to datetime type\n","df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n","\n","# Convert InvoiceDate to datetime and extract only the date\n","df['InvoiceDay'] = df['InvoiceDate'].dt.date"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:31.717294Z","iopub.status.busy":"2023-09-27T15:52:31.71697Z","iopub.status.idle":"2023-09-27T15:52:32.152867Z","shell.execute_reply":"2023-09-27T15:52:32.151667Z","shell.execute_reply.started":"2023-09-27T15:52:31.717265Z"},"trusted":true},"outputs":[],"source":["# Find the most recent purchase date for each customer\n","customer_data = df.groupby('CustomerID')['InvoiceDay'].max().reset_index()\n","\n","\n","# Find the most recent date in the entire dataset\n","most_recent_date = df['InvoiceDay'].max()\n","\n","# Convert InvoiceDay to datetime type before subtraction\n","customer_data['InvoiceDay'] = pd.to_datetime(customer_data['InvoiceDay'])\n","most_recent_date = pd.to_datetime(most_recent_date)\n","\n","# Calculate the number of days since the last purchase for each customer\n","customer_data['Days_Since_Last_Purchase'] = (most_recent_date - customer_data['InvoiceDay']).dt.days\n","\n","# Remove the InvoiceDay column\n","customer_data.drop(columns=['InvoiceDay'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:32.154526Z","iopub.status.busy":"2023-09-27T15:52:32.154236Z","iopub.status.idle":"2023-09-27T15:52:32.165485Z","shell.execute_reply":"2023-09-27T15:52:32.163785Z","shell.execute_reply.started":"2023-09-27T15:52:32.1545Z"},"trusted":true},"outputs":[],"source":["customer_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["Note:\n","I've named the\n","customer-centric dataframe as customer_data, which will eventually contain all the customer-based features we plan to create.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### 4-1-2 Frequency (F)\n","\n","<b>In this step, I am going to create two features that quantify the frequency of a customer's engagement with the retailer:<B>\n","\n","<B>Total Transactions:\n","    \n","    This feature represents the total number of transactions made by a customer. It helps in understanding the engagement level of a customer with the retailer.\n","    \n","    \n","<B>Total Products Purchased:<B>\n","    \n","    This feature indicates the total number of products (sum of quantities) purchased by a customer across all transactions. It gives an insight into the customer's buying behavior in terms of the volume of products purchased.\n","    \n","    \n","These features will be crucial in segmenting customers based on their buying frequency, which is a key aspect in determining customer segments for targeted marketing and personalized recommendations."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:32.168111Z","iopub.status.busy":"2023-09-27T15:52:32.167453Z","iopub.status.idle":"2023-09-27T15:52:32.246524Z","shell.execute_reply":"2023-09-27T15:52:32.245521Z","shell.execute_reply.started":"2023-09-27T15:52:32.168062Z"},"trusted":true},"outputs":[],"source":["# Calculate the total number of transactions made by each customer\n","total_transactions = df.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()\n","total_transactions.rename(columns={'InvoiceNo': 'Total_Transactions'}, inplace=True)\n","\n","# Calculate the total number of products purchased by each customer\n","total_products_purchased = df.groupby('CustomerID')['Quantity'].sum().reset_index()\n","total_products_purchased.rename(columns={'Quantity': 'Total_Products_Purchased'}, inplace=True)\n","\n","# Merge the new features into the customer_data dataframe\n","customer_data = pd.merge(customer_data, total_transactions, on='CustomerID')\n","customer_data = pd.merge(customer_data, total_products_purchased, on='CustomerID')\n","\n","# Display the first few rows of the customer_data dataframe\n","customer_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["### 4-1-3 Monetary (M)\n","\n","<B>In this step, I am going to create two features that represent the monetary aspect of customer's transactions:<B>\n","\n","<B>Total Spend:<B>\n","    \n","    This feature represents the total amount of money spent by each customer. It is calculated as the sum of the product of UnitPrice and Quantity for all transactions made by a customer. This feature is crucial as it helps in identifying the total revenue generated by each customer, which is a direct indicator of a customer's value to the business.\n","    \n","<B>Average Transaction Value: <B>\n","    \n","    This feature is calculated as the Total Spend divided by the Total Transactions for each customer. It indicates the average value of a transaction carried out by a customer. This metric is useful in understanding the spending behavior of customers per transaction, which can assist in tailoring marketing strategies and offers to different customer segments based on their average spending patterns.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:32.248483Z","iopub.status.busy":"2023-09-27T15:52:32.248174Z","iopub.status.idle":"2023-09-27T15:52:32.284071Z","shell.execute_reply":"2023-09-27T15:52:32.282965Z","shell.execute_reply.started":"2023-09-27T15:52:32.248455Z"},"trusted":true},"outputs":[],"source":["# Calculate the total spend by each customer\n","df['Total_Spend'] = df['UnitPrice'] * df['Quantity']\n","total_spend = df.groupby('CustomerID')['Total_Spend'].sum().reset_index()\n","\n","# Calculate the average transaction value for each customer\n","average_transaction_value = total_spend.merge(total_transactions, on='CustomerID')\n","average_transaction_value['Average_Transaction_Value'] = average_transaction_value['Total_Spend'] / average_transaction_value['Total_Transactions']\n","\n","# Merge the new features into the customer_data dataframe\n","customer_data = pd.merge(customer_data, total_spend, on='CustomerID')\n","customer_data = pd.merge(customer_data, average_transaction_value[['CustomerID', 'Average_Transaction_Value']], on='CustomerID')\n","\n","# Display the first few rows of the customer_data dataframe\n","customer_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["# 4-2 Product Diversity\n","\n","\n","In this step, we are going to understand the diversity in the product purchase behavior of customers. Understanding product diversity can help in crafting personalized marketing strategies and product recommendations. Here, I am going to define the following feature:\n","\n","<B>Unique Products Purchased:<B>\n","\n","This feature represents the number of distinct products bought by a customer. A higher value indicates that the customer has a diverse taste or preference, buying a wide range of products, while a lower value might indicate a focused or specific preference. Understanding the diversity in product purchases can help in segmenting customers based on their buying diversity, which can be a critical input in personalizing product recommendations."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:32.285949Z","iopub.status.busy":"2023-09-27T15:52:32.285551Z","iopub.status.idle":"2023-09-27T15:52:32.401995Z","shell.execute_reply":"2023-09-27T15:52:32.401024Z","shell.execute_reply.started":"2023-09-27T15:52:32.285917Z"},"trusted":true},"outputs":[],"source":["# Calculate the number of unique products purchased by each customer\n","unique_products_purchased = df.groupby('CustomerID')['StockCode'].nunique().reset_index()\n","unique_products_purchased.rename(columns={'StockCode': 'Unique_Products_Purchased'}, inplace=True)\n","\n","# Merge the new feature into the customer_data dataframe\n","customer_data = pd.merge(customer_data, unique_products_purchased, on='CustomerID')\n","\n","# Display the first few rows of the customer_data dataframe\n","customer_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["## 4-3 Behavioral Features\n","\n","In this step, we aim to understand and capture the shopping patterns and behaviors of customers. These features will give us insights into the customers' preferences regarding when they like to shop, which can be crucial information for personalizing their shopping experience. Here are the features I am planning to introduce:\n","\n","<b>Average Days Between Purchases:<B>\n","\n","This feature represents the average number of days a customer waits before making another purchase. Understanding this can help in predicting when the customer is likely to make their next purchase, which can be a crucial metric for targeted marketing and personalized promotions.\n","\n","<B>Favorite Shopping Day:<B>\n","\n","This denotes the day of the week when the customer shops the most. This information can help in identifying the preferred shopping days of different customer segments, which can be used to optimize marketing strategies and promotions for different days of the week.\n","\n","<B>Favorite Shopping Hour: <b>\n","\n","This refers to the hour of the day when the customer shops the most. Identifying the favorite shopping hour can aid in optimizing the timing of marketing campaigns and promotions to align with the times when different customer segments are most active.\n","By including these behavioral features in our dataset, we can create a more rounded view of our customers, which will potentially enhance the effectiveness of the clustering algorithm, leading to more meaningful customer segments."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:32.406095Z","iopub.status.busy":"2023-09-27T15:52:32.405747Z","iopub.status.idle":"2023-09-27T15:52:36.601768Z","shell.execute_reply":"2023-09-27T15:52:36.600565Z","shell.execute_reply.started":"2023-09-27T15:52:32.406066Z"},"trusted":true},"outputs":[],"source":["# Extract day of week and hour from InvoiceDate\n","df['Day_Of_Week'] = df['InvoiceDate'].dt.dayofweek\n","df['Hour'] = df['InvoiceDate'].dt.hour\n","\n","# Calculate the average number of days between consecutive purchases\n","days_between_purchases = df.groupby('CustomerID')['InvoiceDay'].apply(lambda x: (x.diff().dropna()).apply(lambda y: y.days))\n","average_days_between_purchases = days_between_purchases.groupby('CustomerID').mean().reset_index()\n","average_days_between_purchases.rename(columns={'InvoiceDay': 'Average_Days_Between_Purchases'}, inplace=True)\n","\n","\n","# Find the favorite shopping day of the week\n","favorite_shopping_day = df.groupby(['CustomerID', 'Day_Of_Week']).size().reset_index(name='Count')\n","favorite_shopping_day = favorite_shopping_day.loc[favorite_shopping_day.groupby('CustomerID')['Count'].idxmax()][['CustomerID', 'Day_Of_Week']]\n","\n","\n","# Find the favorite shopping hour of the day\n","favorite_shopping_hour = df.groupby(['CustomerID', 'Hour']).size().reset_index(name='Count')\n","favorite_shopping_hour = favorite_shopping_hour.loc[favorite_shopping_hour.groupby('CustomerID')['Count'].idxmax()][['CustomerID', 'Hour']]\n","\n","# Merge the new features into the customer_data dataframe\n","customer_data = pd.merge(customer_data, average_days_between_purchases, on='CustomerID')\n","customer_data = pd.merge(customer_data, favorite_shopping_day, on='CustomerID')\n","customer_data = pd.merge(customer_data, favorite_shopping_hour, on='CustomerID')\n","\n","# Display the first few rows of the customer_data dataframe\n","customer_data.head()\n"]},{"cell_type":"markdown","metadata":{},"source":["## 4-4 Geographic Features\n","\n","In this step, we will introduce a geographic feature that reflects the geographical location of customers. Understanding the geographic distribution of customers is pivotal for several reasons:\n","\n","<B>Country:<B> \n","    \n","   \n","This feature identifies the country where each customer is located. Including the country data can help us understand region-specific buying patterns and preferences. Different regions might have varying preferences and purchasing behaviors which can be critical in personalizing marketing strategies and inventory planning. Furthermore, it can be instrumental in logistics and supply chain optimization, particularly for an online retailer where shipping and delivery play a significant role."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:36.603966Z","iopub.status.busy":"2023-09-27T15:52:36.603555Z","iopub.status.idle":"2023-09-27T15:52:36.635881Z","shell.execute_reply":"2023-09-27T15:52:36.634821Z","shell.execute_reply.started":"2023-09-27T15:52:36.603932Z"},"trusted":true},"outputs":[],"source":["df['Country'].value_counts(normalize=True).head()"]},{"cell_type":"markdown","metadata":{},"source":["Inference:\n","Given that a substantial portion (89%) of transactions are originating from the United Kingdom, we might consider creating a binary feature indicating whether the transaction is from the UK or not. This approach can potentially streamline the clustering process without losing critical geographical information, especially when considering the application of algorithms like K-means which are sensitive to the dimensionality of the feature space."]},{"cell_type":"markdown","metadata":{},"source":["<B>Methodology:<B>\n","\n","\n","First, I will group the data by CustomerID and Country and calculate the number of transactions per country for each customer.\n","\n","Next, I will identify the main country for each customer (the country from which they have the maximum transactions).\n","\n","Then, I will create a binary column indicating whether the customer is from the UK or not.\n","\n","Finally, I will merge this information with the customer_data dataframe to include the new feature in our analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:36.638849Z","iopub.status.busy":"2023-09-27T15:52:36.638588Z","iopub.status.idle":"2023-09-27T15:52:36.711228Z","shell.execute_reply":"2023-09-27T15:52:36.709842Z","shell.execute_reply.started":"2023-09-27T15:52:36.638827Z"},"trusted":true},"outputs":[],"source":["# Group by CustomerID and Country to get the number of transactions per country for each customer\n","customer_country = df.groupby(['CustomerID', 'Country']).size().reset_index(name='Number_of_Transactions')\n","\n","# Get the country with the maximum number of transactions for each customer (in case a customer has transactions from multiple countries)\n","customer_main_country = customer_country.sort_values('Number_of_Transactions', ascending=False).drop_duplicates('CustomerID')\n","\n","# Create a binary column indicating whether the customer is from the UK or not\n","customer_main_country['Is_UK'] = customer_main_country['Country'].apply(lambda x: 1 if x == 'United Kingdom' else 0)\n","\n","# Merge this data with our customer_data dataframe\n","customer_data = pd.merge(customer_data, customer_main_country[['CustomerID', 'Is_UK']], on='CustomerID', how='left')\n","\n","# Display the first few rows of the customer_data dataframe\n","customer_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:36.713026Z","iopub.status.busy":"2023-09-27T15:52:36.712676Z","iopub.status.idle":"2023-09-27T15:52:36.720146Z","shell.execute_reply":"2023-09-27T15:52:36.719141Z","shell.execute_reply.started":"2023-09-27T15:52:36.712996Z"},"trusted":true},"outputs":[],"source":["# Display feature distribution\n","customer_data['Is_UK'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["## 4-5 Seasonality & Trends"]},{"cell_type":"markdown","metadata":{},"source":["In this step, I will delve into the seasonality and trends in customers' purchasing behaviors, which can offer invaluable insights for tailoring marketing strategies and enhancing customer satisfaction. Here are the features I am looking to introduce:\n","\n","<b>Monthly_Spending_Mean:<b>\n","    \n","    This is the average amount a customer spends monthly. It helps us gauge the general spending habit of each customer. A higher mean indicates a customer who spends more, potentially showing interest in premium products, whereas a lower mean might indicate a more budget-conscious customer.\n","    \n","    \n","<B>Monthly_Spending_Std:<B>\n","    \n","    This feature indicates the variability in a customer's monthly spending. A higher value signals that the customer's spending fluctuates significantly month-to-month, perhaps indicating sporadic large purchases. In contrast, a lower value suggests more stable, consistent spending habits. Understanding this variability can help in crafting personalized promotions or discounts during periods they are expected to spend more.\n","    \n","    \n","<B>Spending_Trend:<b>\n","    \n","    This reflects the trend in a customer's spending over time, calculated as the slope of the linear trend line fitted to their spending data. A positive value indicates an increasing trend in spending, possibly pointing to growing loyalty or satisfaction. Conversely, a negative trend might signal decreasing interest or satisfaction, highlighting a need for re-engagement strategies. A near-zero value signifies stable spending habits. Recognizing these trends can help in developing strategies to either maintain or alter customer spending patterns, enhancing the effectiveness of marketing campaigns.\n","\n","    \n","    <B> By incorporating these detailed insights into our customer segmentation model, we can create more precise and actionable customer groups, facilitating the development of highly targeted marketing strategies and promotions.<B>\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:36.722218Z","iopub.status.busy":"2023-09-27T15:52:36.721126Z","iopub.status.idle":"2023-09-27T15:52:37.249625Z","shell.execute_reply":"2023-09-27T15:52:37.247953Z","shell.execute_reply.started":"2023-09-27T15:52:36.722162Z"},"trusted":true},"outputs":[],"source":["# Extract month and year from InvoiceDate\n","df['Year'] = df['InvoiceDate'].dt.year\n","df['Month'] = df['InvoiceDate'].dt.month\n","\n","# Calculate monthly spending for each customer\n","monthly_spending = df.groupby(['CustomerID', 'Year', 'Month'])['Total_Spend'].sum().reset_index()\n","\n","# Calculate Seasonal Buying Patterns: We are using monthly frequency as a proxy for seasonal buying patterns\n","seasonal_buying_patterns = monthly_spending.groupby('CustomerID')['Total_Spend'].agg(['mean', 'std']).reset_index()\n","seasonal_buying_patterns.rename(columns={'mean': 'Monthly_Spending_Mean', 'std': 'Monthly_Spending_Std'}, inplace=True)\n","\n","# Replace NaN values in Monthly_Spending_Std with 0, implying no variability for customers with single transaction month\n","seasonal_buying_patterns['Monthly_Spending_Std'].fillna(0, inplace=True)\n","\n","# Calculate Trends in Spending \n","# We are using the slope of the linear trend line fitted to the customer's spending over time as an indicator of spending trends\n","def calculate_trend(spend_data):\n","    # If there are more than one data points, we calculate the trend using linear regression\n","    if len(spend_data) > 1:\n","        x = np.arange(len(spend_data))\n","        slope, _, _, _, _ = linregress(x, spend_data)\n","        return slope\n","    # If there is only one data point, no trend can be calculated, hence we return 0\n","    else:\n","        return 0\n","\n","# Apply the calculate_trend function to find the spending trend for each customer\n","spending_trends = monthly_spending.groupby('CustomerID')['Total_Spend'].apply(calculate_trend).reset_index()\n","spending_trends.rename(columns={'Total_Spend': 'Spending_Trend'}, inplace=True)\n","\n","# Merge the new features into the customer_data dataframe\n","customer_data = pd.merge(customer_data, seasonal_buying_patterns, on='CustomerID')\n","customer_data = pd.merge(customer_data, spending_trends, on='CustomerID')\n","\n","# Display the first few rows of the customer_data dataframe\n","customer_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["We've done a great job so far! We have created a dataset that focuses on our customers, using a variety of new features that give us a deeper understanding of their buying patterns and preferences."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:37.25429Z","iopub.status.busy":"2023-09-27T15:52:37.253951Z","iopub.status.idle":"2023-09-27T15:52:37.288309Z","shell.execute_reply":"2023-09-27T15:52:37.286568Z","shell.execute_reply.started":"2023-09-27T15:52:37.254261Z"},"trusted":true},"outputs":[],"source":["# Changing the data type of 'CustomerID' to string as it is a unique identifier and not used in mathematical operations\n","customer_data['CustomerID'] = customer_data['CustomerID'].astype(str)\n","\n","# Convert data types of columns to optimal types\n","customer_data = customer_data.convert_dtypes()\n","customer_data.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:37.290242Z","iopub.status.busy":"2023-09-27T15:52:37.289905Z","iopub.status.idle":"2023-09-27T15:52:37.30527Z","shell.execute_reply":"2023-09-27T15:52:37.30382Z","shell.execute_reply.started":"2023-09-27T15:52:37.290213Z"},"trusted":true},"outputs":[],"source":["customer_data.info()"]},{"cell_type":"markdown","metadata":{},"source":["# 5 - Outlier Detection and Treatment\n","\n","In this section, I will identify and handle outliers in our dataset. Outliers are data points that are significantly different from the majority of other points in the dataset. These points can potentially skew the results of our analysis, especially in k-means clustering where they can significantly influence the position of the cluster centroids. Therefore, it is essential to identify and treat these outliers appropriately to achieve more accurate and meaningful clustering results.\n","\n","Given the multi-dimensional nature of the data, it would be prudent to use algorithms that can detect outliers in multi-dimensional spaces. I am going to use the Isolation Forest algorithm for this task. This algorithm works well for multi-dimensional data and is computationally efficient. It isolates observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n","\n","Let's proceed with this approach:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:37.307122Z","iopub.status.busy":"2023-09-27T15:52:37.306823Z","iopub.status.idle":"2023-09-27T15:52:37.686155Z","shell.execute_reply":"2023-09-27T15:52:37.685052Z","shell.execute_reply.started":"2023-09-27T15:52:37.307094Z"},"trusted":true},"outputs":[],"source":["# Initializing the IsolationForest model with a contamination parameter of 0.05\n","model = IsolationForest(contamination=0.05, random_state=0)\n","\n","# Fitting the model on our dataset (converting DataFrame to NumPy to avoid warning)\n","customer_data['Outlier_Scores'] = model.fit_predict(customer_data.iloc[:, 1:].to_numpy())\n","\n","# Creating a new column to identify outliers (1 for inliers and -1 for outliers)\n","customer_data['Is_Outlier'] = [1 if x == -1 else 0 for x in customer_data['Outlier_Scores']]\n","\n","# Display the first few rows of the customer_data dataframe\n","customer_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["After applying the Isolation Forest algorithm, we have identified the outliers and marked them in a new column named Is_Outlier. We have also calculated the outlier scores which represent the anomaly score of each record.\n","\n","Now let's visualize the distribution of these scores and the number of inliers and outliers detected by the model:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:37.687492Z","iopub.status.busy":"2023-09-27T15:52:37.687188Z","iopub.status.idle":"2023-09-27T15:52:37.945746Z","shell.execute_reply":"2023-09-27T15:52:37.944494Z","shell.execute_reply.started":"2023-09-27T15:52:37.687466Z"},"trusted":true},"outputs":[],"source":["# Calculate the percentage of inliers and outliers\n","outlier_percentage = customer_data['Is_Outlier'].value_counts(normalize=True) * 100\n","\n","# Plotting the percentage of inliers and outliers\n","plt.figure(figsize=(12, 4))\n","outlier_percentage.plot(kind='barh', color='blue')\n","\n","# Adding the percentage labels on the bars\n","for index, value in enumerate(outlier_percentage):\n","    plt.text(value, index, f'{value:.2f}%', fontsize=15)\n","\n","plt.title('Percentage of Inliers and Outliers')\n","plt.xticks(ticks=np.arange(0, 115, 5))\n","plt.xlabel('Percentage (%)')\n","plt.ylabel('Is Outlier')\n","plt.gca().invert_yaxis()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["<B>Inference:<B>\n","\n","\n","From the above plot, we can observe that about 5% of the customers have been identified as outliers in our dataset. This percentage seems to be a reasonable proportion, not too high to lose a significant amount of data, and not too low to retain potentially noisy data points. It suggests that our isolation forest algorithm has worked well in identifying a moderate percentage of outliers, which will be critical in refining our customer segmentation.\n","    \n","    \n","<B>Strategy:<B>\n","    \n","    \n","Considering the nature of the project (customer segmentation using clustering), it is crucial to handle these outliers to prevent them from affecting the clusters' quality significantly. Therefore, I will separate these outliers for further analysis and remove them from our main dataset to prepare it for the clustering analysis.\n","\n","    \n","    \n","Let's proceed with the following steps:\n","\n","Separate the identified outliers for further analysis and save them as a separate file (optional).\n","Remove the outliers from the main dataset to prevent them from influencing the clustering process.\n","Drop the Outlier_Scores and Is_Outlier columns as they were auxiliary columns used for the outlier detection process.\n","Let's implement these steps:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:37.947577Z","iopub.status.busy":"2023-09-27T15:52:37.947234Z","iopub.status.idle":"2023-09-27T15:52:37.96377Z","shell.execute_reply":"2023-09-27T15:52:37.962455Z","shell.execute_reply.started":"2023-09-27T15:52:37.94755Z"},"trusted":true},"outputs":[],"source":["# Separate the outliers for analysis\n","outliers_data = customer_data[customer_data['Is_Outlier'] == 1]\n","\n","# Remove the outliers from the main dataset\n","customer_data_cleaned = customer_data[customer_data['Is_Outlier'] == 0]\n","\n","# Drop the 'Outlier_Scores' and 'Is_Outlier' columns\n","customer_data_cleaned = customer_data_cleaned.drop(columns=['Outlier_Scores', 'Is_Outlier'])\n","\n","# Reset the index of the cleaned data\n","customer_data_cleaned.reset_index(drop=True, inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["We have successfully separated the outliers for further analysis and cleaned our main dataset by removing these outliers. This cleaned dataset is now ready for the next steps in our customer segmentation project, which includes scaling the features and applying clustering algorithms to identify distinct customer segments."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:37.965463Z","iopub.status.busy":"2023-09-27T15:52:37.965118Z","iopub.status.idle":"2023-09-27T15:52:37.975272Z","shell.execute_reply":"2023-09-27T15:52:37.973773Z","shell.execute_reply.started":"2023-09-27T15:52:37.965429Z"},"trusted":true},"outputs":[],"source":["customer_data_cleaned.shape[0]"]},{"cell_type":"markdown","metadata":{},"source":["# 6 - Correlation Analysis\n","\n","\n","Before we proceed to KMeans clustering, it's essential to check the correlation between features in our dataset. The presence of multicollinearity, where features are highly correlated, can potentially affect the clustering process by not allowing the model to learn the actual underlying patterns in the data, as the features do not provide unique information. This could lead to clusters that are not well-separated and meaningful.\n","\n","If we identify multicollinearity, we can utilize dimensionality reduction techniques like PCA. These techniques help in neutralizing the effect of multicollinearity by transforming the correlated features into a new set of uncorrelated variables, preserving most of the original data's variance. This step not only enhances the quality of clusters formed but also makes the clustering process more computationally efficient.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:37.977118Z","iopub.status.busy":"2023-09-27T15:52:37.976833Z","iopub.status.idle":"2023-09-27T15:52:38.666498Z","shell.execute_reply":"2023-09-27T15:52:38.665105Z","shell.execute_reply.started":"2023-09-27T15:52:37.977094Z"},"trusted":true},"outputs":[],"source":["# Reset background style\n","sns.set_style('whitegrid')\n","\n","# Calculate the correlation matrix excluding the 'CustomerID' column\n","corr = customer_data_cleaned.drop(columns=['CustomerID']).corr()\n","\n","# Define a custom colormap\n","colors = ['blue', 'red', 'white', 'green', '#ff6200']\n","my_cmap = LinearSegmentedColormap.from_list('custom_map', colors, N=256)\n","\n","# Create a mask to only show the lower triangle of the matrix (since it's mirrored around its \n","# top-left to bottom-right diagonal)\n","mask = np.zeros_like(corr)\n","mask[np.triu_indices_from(mask, k=1)] = True\n","\n","# Plot the heatmap\n","plt.figure(figsize=(12, 10))\n","sns.heatmap(corr, mask=mask, cmap=my_cmap, annot=True, center=0, fmt='.2f', linewidths=2)\n","plt.title('Correlation Matrix', fontsize=14)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["<B>Inference:<B>\n","    \n","    \n","Looking at the heatmap, we can see that there are some pairs of variables that have high correlations, for instance:\n","\n","    \n","Monthly_Spending_Mean and Average_Transaction_Value\n","    \n","Total_Spend and Total_Products_Purchased\n","    \n","Total_Transactions and Total_Spend\n","    \n","Cancellation_Rate and Cancellation_Frequency\n","    \n","Total_Transactions and Total_Products_Purchased\n","    \n","    \n","These high correlations indicate that these variables move closely together, implying a degree of multicollinearity.\n","\n","Before moving to the next steps, considering the impact of multicollinearity on KMeans clustering, it might be beneficial to treat this multicollinearity possibly through dimensionality reduction techniques such as PCA to create a set of uncorrelated variables. This will help in achieving more stable clusters during the KMeans clustering process.    "]},{"cell_type":"markdown","metadata":{},"source":["# 7 -  Feature Scaling\n","\n","\n","Before we move forward with the clustering and dimensionality reduction, it's imperative to scale our features. This step holds significant importance, especially in the context of distance-based algorithms like K-means and dimensionality reduction methods like PCA. Here's why:\n","\n","\n","For K-means Clustering: K-means relies heavily on the concept of 'distance' between data points to form clusters. When features are not on a similar scale, features with larger values can disproportionately influence the clustering outcome, potentially leading to incorrect groupings.\n","\n","For PCA: PCA aims to find the directions where the data varies the most. When features are not scaled, those with larger values might dominate these components, not accurately reflecting the underlying patterns in the data.\n"]},{"cell_type":"markdown","metadata":{},"source":["<B>Methodology:<b>\n","    \n","    \n","Therefore, to ensure a balanced influence on the model and to reveal the true patterns in the data, I am going to standardize our data, meaning transforming the features to have a mean of 0 and a standard deviation of 1. However, not all features require scaling. Here are the exceptions and the reasons why they are excluded:\n","\n","    \n","<B>CustomerID:<b>\n","    \n","    This feature is just an identifier for the customers and does not contain any meaningful information for clustering.\n","    \n","    \n","<B>Is_UK:<b>\n","    \n","    This is a binary feature indicating whether the customer is from the UK or not. Since it already takes a value of 0 or 1, scaling it won't make any significant difference.\n","    \n","    \n","<B>Day_Of_Week:<B>\n","    \n","    This feature represents the most frequent day of the week that the customer made transactions. Since it's a categorical feature represented by integers (1 to 7), scaling it would not be necessary.\n","    \n","I will proceed to scale the other features in the dataset to prepare it for PCA and K-means clustering."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:38.668348Z","iopub.status.busy":"2023-09-27T15:52:38.668016Z","iopub.status.idle":"2023-09-27T15:52:38.702579Z","shell.execute_reply":"2023-09-27T15:52:38.701698Z","shell.execute_reply.started":"2023-09-27T15:52:38.668318Z"},"trusted":true},"outputs":[],"source":["# Initialize the StandardScaler\n","scaler = StandardScaler()\n","\n","# List of columns that don't need to be scaled\n","columns_to_exclude = ['CustomerID', 'Is_UK', 'Day_Of_Week']\n","\n","# List of columns that need to be scaled\n","columns_to_scale = customer_data_cleaned.columns.difference(columns_to_exclude)\n","\n","# Copy the cleaned dataset\n","customer_data_scaled = customer_data_cleaned.copy()\n","\n","# Applying the scaler to the necessary columns in the dataset\n","customer_data_scaled[columns_to_scale] = scaler.fit_transform(customer_data_scaled[columns_to_scale])\n","\n","# Display the first few rows of the scaled data\n","customer_data_scaled.head()"]},{"cell_type":"markdown","metadata":{},"source":["# 8 -  Dimensionality Reduction\n","\n","<B>Why We Need Dimensionality Reduction?<b>\n","\n","\n","Multicollinearity Detected:\n","    In the previous steps, we identified that our dataset contains multicollinear features. Dimensionality reduction can help us remove redundant information and alleviate the multicollinearity issue.\n","    \n","Better Clustering with K-means:\n","    Since K-means is a distance-based algorithm, having a large number of features can sometimes dilute the meaningful underlying patterns in the data. By reducing the dimensionality, we can help K-means to find more compact and well-separated clusters.\n","    \n","Noise Reduction: \n","    By focusing only on the most important features, we can potentially remove noise in the data, leading to more accurate and stable clusters.\n","    \n","Enhanced Visualization:\n","    In the context of customer segmentation, being able to visualize customer groups in two or three dimensions can provide intuitive insights. Dimensionality reduction techniques can facilitate this by reducing the data to a few principal components which can be plotted easily.\n","    \n","Improved Computational Efficiency:\n","    \n","   Reducing the number of features can speed up the computation time during the modeling process, making our clustering algorithm more efficient.\n","    \n","Let's proceed to select an appropriate dimensionality reduction method to our data."]},{"cell_type":"markdown","metadata":{},"source":["<B>Methodology<B>\n","    \n","   \n","I will apply PCA on all the available components and plot the cumulative variance explained by them. This process will allow me to visualize how much variance each additional principal component can explain, thereby helping me to pinpoint the optimal number of components to retain for the analysis:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:38.704262Z","iopub.status.busy":"2023-09-27T15:52:38.703904Z","iopub.status.idle":"2023-09-27T15:52:39.250871Z","shell.execute_reply":"2023-09-27T15:52:39.249575Z","shell.execute_reply.started":"2023-09-27T15:52:38.704229Z"},"trusted":true},"outputs":[],"source":["# Setting CustomerID as the index column\n","customer_data_scaled.set_index('CustomerID', inplace=True)\n","\n","# Apply PCA\n","pca = PCA().fit(customer_data_scaled)\n","\n","# Calculate the Cumulative Sum of the Explained Variance\n","explained_variance_ratio = pca.explained_variance_ratio_\n","cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n","\n","# Set the optimal k value (based on our analysis, we can choose 6)\n","optimal_k = 6\n","\n","# Set seaborn plot style\n","sns.set(rc={'axes.facecolor': '#fcf0dc'}, style='darkgrid')\n","\n","# Plot the cumulative explained variance against the number of components\n","plt.figure(figsize=(20, 10))\n","\n","\n","# Bar chart for the explained variance of each component\n","barplot = sns.barplot(x=list(range(1, len(cumulative_explained_variance) + 1)),\n","                      y=explained_variance_ratio,\n","                      color='#fcc36d',\n","                      alpha=0.8)\n","\n","# Line plot for the cumulative explained variance\n","lineplot, = plt.plot(range(0, len(cumulative_explained_variance)), cumulative_explained_variance,\n","                     marker='o', linestyle='--', color='#ff6200', linewidth=2)\n","\n","# Plot optimal k value line\n","optimal_k_line = plt.axvline(optimal_k - 1, color='red', linestyle='--', label=f'Optimal k value = {optimal_k}') \n","\n","# Set labels and title\n","plt.xlabel('Number of Components', fontsize=14)\n","plt.ylabel('Explained Variance', fontsize=14)\n","plt.title('Cumulative Variance vs. Number of Components', fontsize=18)\n","\n","\n","# Customize ticks and legend\n","plt.xticks(range(0, len(cumulative_explained_variance)))\n","plt.legend(handles=[barplot.patches[0], lineplot, optimal_k_line],\n","           labels=['Explained Variance of Each Component', 'Cumulative Explained Variance', f'Optimal k value = {optimal_k}'],\n","           loc=(0.62, 0.1),\n","           frameon=True,\n","           framealpha=1.0,  \n","           edgecolor='#ff6200')  \n","\n","# Display the variance values for both graphs on the plots\n","x_offset = -0.3\n","y_offset = 0.01\n","for i, (ev_ratio, cum_ev_ratio) in enumerate(zip(explained_variance_ratio, cumulative_explained_variance)):\n","    plt.text(i, ev_ratio, f\"{ev_ratio:.2f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n","    if i > 0:\n","        plt.text(i + x_offset, cum_ev_ratio + y_offset, f\"{cum_ev_ratio:.2f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n","\n","plt.grid(axis='both')   \n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["<B>Conclusion<b>\n","The plot and the cumulative explained variance values indicate how much of the total variance in the dataset is captured by each principal component, as well as the cumulative variance explained by the first n components.\n","\n","Here, we can observe that:\n","\n","The first component explains approximately 28% of the variance.\n","\n","The first two components together explain about 49% of the variance.\n","\n","The first three components explain approximately 61% of the variance, and so on.\n","\n","To choose the optimal number of components, we generally look for a point where adding another component doesn't significantly increase the cumulative explained variance, often referred to as the \"elbow point\" in the curve.\n","\n","From the plot, we can see that the increase in cumulative variance starts to slow down after the 6th component (which captures about 81% of the total variance).\n","\n","Considering the context of customer segmentation, we want to retain a sufficient amount of information to identify distinct customer groups effectively. Therefore, retaining the first 6 components might be a balanced choice, as they together explain a substantial portion of the total variance while reducing the dimensionality of the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:39.252943Z","iopub.status.busy":"2023-09-27T15:52:39.251939Z","iopub.status.idle":"2023-09-27T15:52:39.284033Z","shell.execute_reply":"2023-09-27T15:52:39.283252Z","shell.execute_reply.started":"2023-09-27T15:52:39.25289Z"},"trusted":true},"outputs":[],"source":["# Creating a PCA object with 6 components\n","pca = PCA(n_components=6)\n","\n","# Fitting and transforming the original data to the new PCA dataframe\n","customer_data_pca = pca.fit_transform(customer_data_scaled)\n","\n","# Creating a new dataframe from the PCA dataframe, with columns labeled PC1, PC2, etc.\n","customer_data_pca = pd.DataFrame(customer_data_pca, columns=['PC'+str(i+1) for i in range(pca.n_components_)])\n","\n","# Adding the CustomerID index back to the new PCA dataframe\n","customer_data_pca.index = customer_data_scaled.index"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:39.285651Z","iopub.status.busy":"2023-09-27T15:52:39.284993Z","iopub.status.idle":"2023-09-27T15:52:39.303Z","shell.execute_reply":"2023-09-27T15:52:39.301927Z","shell.execute_reply.started":"2023-09-27T15:52:39.285624Z"},"trusted":true},"outputs":[],"source":["# Displaying the resulting dataframe based on the PCs\n","customer_data_pca.head()"]},{"cell_type":"markdown","metadata":{},"source":["Now, let's extract the coefficients corresponding to each principal component to better understand the transformation performed by PCA:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:39.305015Z","iopub.status.busy":"2023-09-27T15:52:39.304209Z","iopub.status.idle":"2023-09-27T15:52:39.435971Z","shell.execute_reply":"2023-09-27T15:52:39.435066Z","shell.execute_reply.started":"2023-09-27T15:52:39.304989Z"},"trusted":true},"outputs":[],"source":["# Define a function to highlight the top 3 absolute values in each column of a dataframe\n","def highlight_top3(column):\n","    top3 = column.abs().nlargest(3).index\n","    return ['background-color:  #ffeacc' if i in top3 else '' for i in column.index]\n","\n","# Create the PCA component DataFrame and apply the highlighting function\n","pc_df = pd.DataFrame(pca.components_.T, columns=['PC{}'.format(i+1) for i in range(pca.n_components_)],  \n","                     index=customer_data_scaled.columns)\n","\n","pc_df.style.apply(highlight_top3, axis=0)"]},{"cell_type":"markdown","metadata":{},"source":["# 9 -  K-Means Clustering"]},{"cell_type":"markdown","metadata":{},"source":["K-Means:\n","K-Means is an unsupervised machine learning algorithm that clusters data into a specified number of groups (K) by minimizing the within-cluster sum-of-squares (WCSS), also known as inertia. The algorithm iteratively assigns each data point to the nearest centroid, then updates the centroids by calculating the mean of all assigned points. The process repeats until convergence or a stopping criterion is reached."]},{"cell_type":"markdown","metadata":{},"source":["Drawbacks of K-Means:\n","Here are the main drawbacks of the K-means clustering algorithm and their corresponding solutions:\n","\n","1 Inertia is influenced by the number of dimensions: The value of inertia tends to increase in high-dimensional spaces due to the curse of dimensionality, which can distort the Euclidean distances between data points.\n","Solution: Performing dimensionality reduction, such as PCA, before applying K-means to alleviate this issue and speed up computations.\n","\n","2 Dependence on Initial Centroid Placement: The K-means algorithm might find a local minimum instead of a global minimum, based on where the centroids are initially placed.\n","Solution: To enhance the likelihood of locating the global minimum, we can employ the k-means++ initialization method.\n","\n","3 Requires specifying the number of clusters: K-means requires specifying the number of clusters (K) beforehand, which may not be known in advance.\n","Solution: Using methods such as the elbow method and silhouette analysis to estimate the optimal number of clusters.\n","\n","4 Sensitivity to unevenly sized or sparse clusters: K-means might struggle with clusters of different sizes or densities.\n","Solution: Increasing the number of random initializations (n_init) or consider using algorithms that handle unevenly sized clusters better, like GMM or DBSCAN.\n","\n","5 Assumes convex and isotropic clusters: K-means assumes that clusters are spherical and have similar variances, which is not always the case. It may struggle with elongated or irregularly shaped clusters.\n","Solution: Considering using clustering algorithms that do not make these assumptions, such as DBSCAN or Gaussian Mixture Model (GMM).\n","\n","Taking into account the aforementioned considerations, I initially applied PCA to the dataset. For the KMeans algorithm, I will set the init parameter to k-means++ and n_init to 10. To determine the optimal number of clusters, I will employ the elbow method and silhouette analysis. Additionally, it might be beneficial to explore the use of alternative clustering algorithms such as GMM and DBSCAN in future analyses to potentially enhance the segmentation results."]},{"cell_type":"markdown","metadata":{},"source":["### 9-1 Determining the Optimal Number of Clusters\n","\n","To ascertain the optimal number of clusters (k) for segmenting customers, I will explore two renowned methods:\n","\n","Elbow Method\n","\n","Silhouette Method\n","\n","It's common to utilize both methods in practice to corroborate the results."]},{"cell_type":"markdown","metadata":{},"source":["<B>Elbow Method<B>\n","\n","What is the Elbow Method?\n","The Elbow Method is a technique for identifying the ideal number of clusters in a dataset. It involves iterating through the data, generating clusters for various values of k. The k-means algorithm calculates the sum of squared distances between each data point and its assigned cluster centroid, known as the inertia or WCSS score. By plotting the inertia score against the k value, we create a graph that typically exhibits an elbow shape, hence the name \"Elbow Method\". The elbow point represents the k-value where the reduction in inertia achieved by increasing k becomes negligible, indicating the optimal stopping point for the number of clusters.\n","    \n","\n","Utilizing the YellowBrick Library\n","In this section, I will employ the YellowBrick library to facilitate the implementation of the Elbow method. YellowBrick, an extension of the Scikit-Learn API, is renowned for its ability to rapidly generate insightful visualizations in the field of machine learning.    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:39.437558Z","iopub.status.busy":"2023-09-27T15:52:39.437142Z","iopub.status.idle":"2023-09-27T15:52:56.934457Z","shell.execute_reply":"2023-09-27T15:52:56.93328Z","shell.execute_reply.started":"2023-09-27T15:52:39.437533Z"},"trusted":true},"outputs":[],"source":["# Instantiate the clustering model with the specified parameters\n","km = KMeans(init='k-means++', n_init=10, max_iter=100, random_state=0)\n","\n","# Create a figure and axis with the desired size\n","fig, ax = plt.subplots(figsize=(12, 5))\n","\n","# Instantiate the KElbowVisualizer with the model and range of k values, and disable the timing plot\n","visualizer = KElbowVisualizer(km, k=(2, 15), timings=False, ax=ax)\n","\n","# Fit the data to the visualizer\n","visualizer.fit(customer_data_pca)\n","\n","# Finalize and render the figure\n","visualizer.show();"]},{"cell_type":"markdown","metadata":{},"source":["Optimal k Value: Elbow Method Insights\n","The optimal value of k for the KMeans clustering algorithm can be found at the elbow point. Using the YellowBrick library for the Elbow method, we observe that the suggested optimal k value is 6. However, we don't have a very distinct elbow point in this case, which is common in real-world data. From the plot, we can see that the inertia continues to decrease significantly up to k=6, indicating that the optimum value of k could be between 4 and 8. To choose the best k within this range, we can employ the silhouette analysis, another cluster quality evaluation method. Additionally, incorporating business insights can help determine a practical k value."]},{"cell_type":"markdown","metadata":{},"source":["### 9-1-2 Silhouette Method\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:56.939073Z","iopub.status.busy":"2023-09-27T15:52:56.936617Z","iopub.status.idle":"2023-09-27T15:52:56.951341Z","shell.execute_reply":"2023-09-27T15:52:56.950247Z","shell.execute_reply.started":"2023-09-27T15:52:56.939034Z"},"trusted":true},"outputs":[],"source":["def silhouette_analysis(df, start_k, stop_k, figsize=(15, 16)):\n","    \"\"\"\n","    Perform Silhouette analysis for a range of k values and visualize the results.\n","    \"\"\"\n","\n","    # Set the size of the figure\n","    plt.figure(figsize=figsize)\n","\n","    # Create a grid with (stop_k - start_k + 1) rows and 2 columns\n","    grid = gridspec.GridSpec(stop_k - start_k + 1, 2)\n","\n","    # Assign the first plot to the first row and both columns\n","    first_plot = plt.subplot(grid[0, :])\n","\n","    # First plot: Silhouette scores for different k values\n","    sns.set_palette(['darkorange'])\n","\n","    silhouette_scores = []\n","\n","    # Iterate through the range of k values\n","    for k in range(start_k, stop_k + 1):\n","        km = KMeans(n_clusters=k, init='k-means++', n_init=10, max_iter=100, random_state=0)\n","        km.fit(df)\n","        labels = km.predict(df)\n","        score = silhouette_score(df, labels)\n","        silhouette_scores.append(score)    \n","\n","    best_k = start_k + silhouette_scores.index(max(silhouette_scores))\n","\n","    plt.plot(range(start_k, stop_k + 1), silhouette_scores, marker='o')\n","    plt.xticks(range(start_k, stop_k + 1))\n","    plt.xlabel('Number of clusters (k)')\n","    plt.ylabel('Silhouette score')\n","    plt.title('Average Silhouette Score for Different k Values', fontsize=15)\n","\n","    # Add the optimal k value text to the plot\n","    optimal_k_text = f'The k value with the highest Silhouette score is: {best_k}'\n","    plt.text(10, 0.23, optimal_k_text, fontsize=12, verticalalignment='bottom', \n","             horizontalalignment='left', bbox=dict(facecolor='#fcc36d', edgecolor='#ff6200', boxstyle='round, pad=0.5'))\n","             \n","\n","    # Second plot (subplot): Silhouette plots for each k value\n","    colors = sns.color_palette(\"bright\")\n","\n","    for i in range(start_k, stop_k + 1):    \n","        km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=0)\n","        row_idx, col_idx = divmod(i - start_k, 2)\n","\n","        # Assign the plots to the second, third, and fourth rows\n","        ax = plt.subplot(grid[row_idx + 1, col_idx])\n","\n","        visualizer = SilhouetteVisualizer(km, colors=colors, ax=ax)\n","        visualizer.fit(df)\n","\n","        # Add the Silhouette score text to the plot\n","        score = silhouette_score(df, km.labels_)\n","        ax.text(0.97, 0.02, f'Silhouette Score: {score:.2f}', fontsize=12, \\\n","                ha='right', transform=ax.transAxes, color='red')\n","\n","        ax.set_title(f'Silhouette Plot for {i} Clusters', fontsize=15)\n","\n","    plt.tight_layout()\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T15:52:56.95315Z","iopub.status.busy":"2023-09-27T15:52:56.952572Z"},"trusted":true},"outputs":[],"source":["silhouette_analysis(customer_data_pca, 3, 12, figsize=(20, 50))"]},{"cell_type":"markdown","metadata":{},"source":["Guidelines to Interpret Silhouette Plots and Determine the Optimal K:\n","To interpret silhouette plots and identify the optimal number of clusters (( k )), consider the following criteria:\n","\n","1 Analyze the Silhouette Plots:\n","\n","Silhouette Score Width:\n","\n","Wide Widths (closer to +1): Indicate that the data points in the cluster are well separated from points in other clusters, suggesting well-defined clusters.\n","Narrow Widths (closer to -1): Show that data points in the cluster are not distinctly separated from other clusters, indicating poorly defined clusters.\n","Average Silhouette Score:\n","\n","High Average Width: A cluster with a high average silhouette score indicates well-separated clusters.\n","Low Average Width: A cluster with a low average silhouette score indicates poor separation between clusters.\n","2 Uniformity in Cluster Size:\n","\n","2.1 Cluster Thickness:\n","\n","Uniform Thickness: Indicates that clusters have a roughly equal number of data points, suggesting a balanced clustering structure.\n","Variable Thickness: Signifies an imbalance in the data point distribution across clusters, with some clusters having many data points and others too few.\n","\n","3 Peaks in Average Silhouette Score:\n","Clear Peaks: A clear peak in the average silhouette score plot for a specific ( k ) value indicates this ( k ) might be optimal.\n","4 Minimize Fluctuations in Silhouette Plot Widths:\n","Uniform Widths: Seek silhouette plots with similar widths across clusters, suggesting a more balanced and optimal clustering.\n","Variable Widths: Avoid wide fluctuations in silhouette plot widths, indicating that clusters are not well-defined and may vary in compactness.\n","5 Optimal Cluster Selection:\n","Maximize the Overall Average Silhouette Score: Choose the ( k ) value that gives the highest average silhouette score across all clusters, indicating well-defined clusters.\n","Avoid Below-Average Silhouette Scores: Ensure most clusters have above-average silhouette scores to prevent suboptimal clustering structures.\n","6 Visual Inspection of Silhouette Plots:\n","Consistent Cluster Formation: Visually inspect the silhouette plots for each ( k ) value to evaluate the consistency and structure of the formed clusters.\n","Cluster Compactness: Look for more compact clusters, with data points having silhouette scores closer to +1, indicating better clustering."]},{"cell_type":"markdown","metadata":{},"source":["Optimal k Value: Silhouette Method Insights\n","Based on above guidelines and after carefully considering the silhouette plots, it's clear that choosing ( k = 3 ) is the better option. This choice gives us clusters that are more evenly matched and well-defined, making our clustering solution stronger and more reliable."]},{"cell_type":"markdown","metadata":{},"source":["## 9-2 Clustering Model - K-means\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Apply KMeans clustering using the optimal k\n","kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=100, random_state=0)\n","kmeans.fit(customer_data_pca)\n","\n","# Get the frequency of each cluster\n","cluster_frequencies = Counter(kmeans.labels_)\n","\n","# Create a mapping from old labels to new labels based on frequency\n","label_mapping = {label: new_label for new_label, (label, _) in \n","                 enumerate(cluster_frequencies.most_common())}\n","\n","# Reverse the mapping to assign labels as per your criteria\n","label_mapping = {v: k for k, v in {2: 1, 1: 0, 0: 2}.items()}\n","\n","# Apply the mapping to get the new labels\n","new_labels = np.array([label_mapping[label] for label in kmeans.labels_])\n","\n","# Append the new cluster labels back to the original dataset\n","customer_data_cleaned['cluster'] = new_labels\n","\n","# Append the new cluster labels to the PCA version of the dataset\n","customer_data_pca['cluster'] = new_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Display the first few rows of the original dataframe\n","customer_data_cleaned.head()"]},{"cell_type":"markdown","metadata":{},"source":["# 10- Clustering Evaluation\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"border-radius:10px; padding: 15px; background-color: #ffeacc; font-size:120%; text-align:left\">\n","\n","After determining the optimal number of clusters (which is 3 in our case) using elbow and silhouette analyses, I move onto the evaluation step to assess the quality of the clusters formed. This step is essential to validate the effectiveness of the clustering and to ensure that the clusters are __coherent__ and __well-separated__. The evaluation metrics and a visualization technique I plan to use are outlined below:\n","    \n","- 1 __3D Visualization of Top PCs__ \n","\n","    \n","- 2 __Cluster Distribution Visualization__ \n","    \n","    \n","- 3 __Evaluation Metrics__ \n","    \n","    * Silhouette Score\n","    * Calinski Harabasz Score\n","    * Davies Bouldin Score\n","       \n","____  \n","    \n","**Note**: We are using the PCA version of the dataset for evaluation because this is the space where the clusters were actually formed, capturing the most significant patterns in the data. Evaluating in this space ensures a more accurate representation of the cluster quality, helping us understand the true cohesion and separation achieved during clustering. This approach also aids in creating a clearer 3D visualization using the top principal components, illustrating the actual separation between clusters."]},{"cell_type":"markdown","metadata":{},"source":["### Step 10.1 | 3D Visualization of Top Principal Components"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Setting up the color scheme for the clusters (RGB order)\n","colors = ['#e8000b', '#1ac938', '#023eff']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create separate data frames for each cluster\n","cluster_0 = customer_data_pca[customer_data_pca['cluster'] == 0]\n","cluster_1 = customer_data_pca[customer_data_pca['cluster'] == 1]\n","cluster_2 = customer_data_pca[customer_data_pca['cluster'] == 2]\n","\n","# Create a 3D scatter plot\n","fig = go.Figure()\n","\n","# Add data points for each cluster separately and specify the color\n","fig.add_trace(go.Scatter3d(x=cluster_0['PC1'], y=cluster_0['PC2'], z=cluster_0['PC3'], \n","                           mode='markers', marker=dict(color=colors[0], size=5, opacity=0.4), name='Cluster 0'))\n","fig.add_trace(go.Scatter3d(x=cluster_1['PC1'], y=cluster_1['PC2'], z=cluster_1['PC3'], \n","                           mode='markers', marker=dict(color=colors[1], size=5, opacity=0.4), name='Cluster 1'))\n","fig.add_trace(go.Scatter3d(x=cluster_2['PC1'], y=cluster_2['PC2'], z=cluster_2['PC3'], \n","                           mode='markers', marker=dict(color=colors[2], size=5, opacity=0.4), name='Cluster 2'))\n","\n","# Set the title and layout details\n","fig.update_layout(\n","    title=dict(text='3D Visualization of Customer Clusters in PCA Space', x=0.5),\n","    scene=dict(\n","        xaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white', title='PC1'),\n","        yaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white', title='PC2'),\n","        zaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white', title='PC3'),\n","    ),\n","    width=900,\n","    height=800\n",")\n","\n","# Show the plot\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Step 10.2 | Cluster Distribution Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Calculate the percentage of customers in each cluster\n","cluster_percentage = (customer_data_pca['cluster'].value_counts(normalize=True) * 100).reset_index()\n","cluster_percentage.columns = ['Cluster', 'Percentage']\n","cluster_percentage.sort_values(by='Cluster', inplace=True)\n","\n","# Create a horizontal bar plot\n","plt.figure(figsize=(10, 4))\n","sns.barplot(x='Percentage', y='Cluster', data=cluster_percentage, orient='h', palette=colors)\n","\n","# Adding percentages on the bars\n","for index, value in enumerate(cluster_percentage['Percentage']):\n","    plt.text(value+0.5, index, f'{value:.2f}%')\n","\n","plt.title('Distribution of Customers Across Clusters', fontsize=14)\n","plt.xticks(ticks=np.arange(0, 50, 5))\n","plt.xlabel('Percentage (%)')\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["Inference\n","The distribution of customers across the clusters, as depicted by the bar plot, suggests a fairly balanced distribution with clusters 0 and 1 holding around 41% of customers each and cluster 2 accommodating approximately 18% of the customers.\n","\n","This balanced distribution indicates that our clustering process has been largely successful in identifying meaningful patterns within the data, rather than merely grouping noise or outliers. It implies that each cluster represents a substantial and distinct segment of the customer base, thereby offering valuable insights for future business strategies.\n","\n","Moreover, the fact that no cluster contains a very small percentage of customers, assures us that each cluster is significant and not just representing outliers or noise in the data. This setup allows for a more nuanced understanding and analysis of different customer segments, facilitating effective and informed decision-making"]},{"cell_type":"markdown","metadata":{},"source":["### Step 10.3 | Evaluation Metrics"]},{"cell_type":"markdown","metadata":{},"source":["To further scrutinize the quality of our clustering, I will employ the following metrics:\n","\n","Silhouette Score: A measure to evaluate the separation distance between the clusters. Higher values indicate better cluster separation. It ranges from -1 to 1.\n","Calinski Harabasz Score: This score is used to evaluate the dispersion between and within clusters. A higher score indicates better defined clusters.\n","Davies Bouldin Score: It assesses the average similarity between each cluster and its most similar cluster. Lower values indicate better cluster separation."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Compute number of customers\n","num_observations = len(customer_data_pca)\n","\n","# Separate the features and the cluster labels\n","X = customer_data_pca.drop('cluster', axis=1)\n","clusters = customer_data_pca['cluster']\n","\n","# Compute the metrics\n","sil_score = silhouette_score(X, clusters)\n","calinski_score = calinski_harabasz_score(X, clusters)\n","davies_score = davies_bouldin_score(X, clusters)\n","\n","# Create a table to display the metrics and the number of observations\n","table_data = [\n","    [\"Number of Observations\", num_observations],\n","    [\"Silhouette Score\", sil_score],\n","    [\"Calinski Harabasz Score\", calinski_score],\n","    [\"Davies Bouldin Score\", davies_score]\n","]\n","\n","# Print the table\n","print(tabulate(table_data, headers=[\"Metric\", \"Value\"], tablefmt='pretty'))"]},{"cell_type":"markdown","metadata":{},"source":["Clustering Quality Inference\n","The Silhouette Score of approximately 0.236, although not close to 1, still indicates a fair amount of separation between the clusters. It suggests that the clusters are somewhat distinct, but there might be slight overlaps between them. Generally, a score closer to 1 would be ideal, indicating more distinct and well-separated clusters.\n","The Calinski Harabasz Score is 1257.17, which is considerably high, indicating that the clusters are well-defined. A higher score in this metric generally signals better cluster definitions, thus implying that our clustering has managed to find substantial structure in the data.\n","The Davies Bouldin Score of 1.37 is a reasonable score, indicating a moderate level of similarity between each cluster and its most similar one. A lower score is generally better as it indicates less similarity between clusters, and thus, our score here suggests a decent separation between the clusters.\n","In conclusion, the metrics suggest that the clustering is of good quality, with clusters being well-defined and fairly separated. However, there might still be room for further optimization to enhance cluster separation and definition, potentially by trying other clustering and dimensionality reduction algorithms.\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Step 11 | Cluster Analysis and Profiling\n","\n","\n","\n","In this section, I am going to analyze the characteristics of each cluster to understand the distinct behaviors and preferences of different customer segments and also profile each cluster to identify the key traits that define the customers in each cluster.\n","\n","\n","\n","\n","Step 11.1 | Radar Chart Approach\n"," Tabel of Contents\n","\n","First of all, I am going to create radar charts to visualize the centroid values of each cluster across different features. This can give a quick visual comparison of the profiles of different clusters.To construct the radar charts, it's essential to first compute the centroid for each cluster. This centroid represents the mean value for all features within a specific cluster. Subsequently, I will display these centroids on the radar charts, facilitating a clear visualization of the central tendencies of each feature across the various clusters:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Setting 'CustomerID' column as index and assigning it to a new dataframe\n","df_customer = customer_data_cleaned.set_index('CustomerID')\n","\n","# Standardize the data (excluding the cluster column)\n","scaler = StandardScaler()\n","df_customer_standardized = scaler.fit_transform(df_customer.drop(columns=['cluster'], axis=1))\n","\n","# Create a new dataframe with standardized values and add the cluster column back\n","df_customer_standardized = pd.DataFrame(df_customer_standardized, columns=df_customer.columns[:-1], index=df_customer.index)\n","df_customer_standardized['cluster'] = df_customer['cluster']\n","\n","# Calculate the centroids of each cluster\n","cluster_centroids = df_customer_standardized.groupby('cluster').mean()\n","\n","# Function to create a radar chart\n","def create_radar_chart(ax, angles, data, color, cluster):\n","    # Plot the data and fill the area\n","    ax.fill(angles, data, color=color, alpha=0.4)\n","    ax.plot(angles, data, color=color, linewidth=2, linestyle='solid')\n","    \n","    # Add a title\n","    ax.set_title(f'Cluster {cluster}', size=20, color=color, y=1.1)\n","\n","# Set data\n","labels=np.array(cluster_centroids.columns)\n","num_vars = len(labels)\n","\n","# Compute angle of each axis\n","angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n","\n","# The plot is circular, so we need to \"complete the loop\" and append the start to the end\n","labels = np.concatenate((labels, [labels[0]]))\n","angles += angles[:1]\n","\n","# Initialize the figure\n","fig, ax = plt.subplots(figsize=(20, 10), subplot_kw=dict(polar=True), nrows=1, ncols=3)\n","\n","# Create radar chart for each cluster\n","for i, color in enumerate(colors):\n","    data = cluster_centroids.loc[i].tolist()\n","    data += data[:1]  # Complete the loop\n","    create_radar_chart(ax[i], angles, data, color, i)\n","\n","# Add input data\n","ax[0].set_xticks(angles[:-1])\n","ax[0].set_xticklabels(labels[:-1])\n","\n","ax[1].set_xticks(angles[:-1])\n","ax[1].set_xticklabels(labels[:-1])\n","\n","ax[2].set_xticks(angles[:-1])\n","ax[2].set_xticklabels(labels[:-1])\n","\n","# Add a grid\n","ax[0].grid(color='grey', linewidth=0.5)\n","\n","# Display the plot\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Customer Profiles Derived from Radar Chart Analysis\n","\n","\n","<B>Cluster 0 (Red Chart):<B>\n","    \n","    \n"," Profile: Sporadic Shoppers with a Preference for Weekend Shopping\n","\n","Customers in this cluster tend to spend less, with a lower number of transactions and products purchased.\n","They have a slight tendency to shop during the weekends, as indicated by the very high Day_of_Week value.\n","Their spending trend is relatively stable but on the lower side, and they have a low monthly spending variation (low Monthly_Spending_Std).\n","These customers have not engaged in many cancellations, showing a low cancellation frequency and rate.\n","The average transaction value is on the lower side, indicating that when they do shop, they tend to spend less per transaction.\n","\n","\n","<B>Cluster 1 (Green Chart):<B>\n","    \n","    \n"," Profile: Infrequent Big Spenders with a High Spending Trend\n","\n","Customers in this cluster show a moderate level of spending, but their transactions are not very frequent, as indicated by the high Days_Since_Last_Purchase and Average_Days_Between_Purchases.\n","They have a very high spending trend, indicating that their spending has been increasing over time.\n","These customers prefer shopping late in the day, as indicated by the high Hour value, and they mainly reside in the UK.\n","They have a tendency to cancel a moderate number of transactions, with a medium cancellation frequency and rate.\n","Their average transaction value is relatively high, meaning that when they shop, they tend to make substantial purchases.\n","\n","\n","<B>Cluster 2 (Blue Chart):<B>\n","    \n","    \n"," Profile: Frequent High-Spenders with a High Rate of Cancellations\n","\n","Customers in this cluster are high spenders with a very high total spend, and they purchase a wide variety of unique products.\n","They engage in frequent transactions, but also have a high cancellation frequency and rate.\n","These customers have a very low average time between purchases, and they tend to shop early in the day (low Hour value).\n","Their monthly spending shows high variability, indicating that their spending patterns might be less predictable compared to other clusters.\n","Despite their high spending, they show a low spending trend, suggesting that their high spending levels might be decreasing over time."]},{"cell_type":"markdown","metadata":{},"source":["In the final phase of this project, I am set to develop a recommendation system to enhance the online shopping experience. This system will suggest products to customers based on the purchasing patterns prevalent in their respective clusters. Earlier in the project, during the customer data preparation stage, I isolated a small fraction (5%) of the customers identified as outliers and reserved them in a separate dataset called outliers_data.\n","\n","Now, focusing on the core 95% of the customer group, I analyze the cleansed customer data to pinpoint the top-selling products within each cluster. Leveraging this information, the system will craft personalized recommendations, suggesting the top three products popular within their cluster that they have not yet purchased. This not only facilitates targeted marketing strategies but also enriches the personal shopping experience, potentially boosting sales. For the outlier group, a basic approach could be to recommend random products, as a starting point to engage them."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
